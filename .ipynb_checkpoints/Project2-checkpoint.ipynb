{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6509a1c",
   "metadata": {},
   "source": [
    "# How do various factors like region, fans owned, etc. affect people's sentiments and polarization on social media during the 2020 US election?\n",
    "### Zihao(Gavin) Zou, 1006668429, zihao.zou@mail.utoronto.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9224d0f",
   "metadata": {},
   "source": [
    "# 1. Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af909b",
   "metadata": {},
   "source": [
    "## 1.1 Introduction\n",
    "\n",
    "Social media posts have been the target of social research for a long time. With the utilization of computer programs and AI databases, researchers can gain more insights from the posts. In this report, I explore how various factors can show a predictive effect on people's sentiments and polarization during the 2020 US Presidential election using tweets. The Y variable is the polarization score, while the X variables are state, followers, user join date, country, and tweet posted time. \n",
    "\n",
    "The polarization score is calculated using the tool pack called TextBlob. TextBlob connects to a large natural language processing model that is continuously developed. This model takes in a sentence and calculates the polarization score that ranges from -1 to 1, where a negative one shows extremely assertive, aggressive or hostile, while one indicates extremely approachable or positive. This score is believed to identify people's sentiments and polarization accurately, so I am using data generated by this tool pack in this research.\n",
    "\n",
    "For the five X variables I choose, the reasonings are as below. First, the state shows how the location within the United States affects what people discuss. Second, the country shows how international regions will affect topic popularities. Third, join data is an implication of a person's internet experience, and we observe how that could affect people's choice of topics. Fourth, followers show how social media influence can affect people's participation in different issues. Last, created date identifies how topics trend over a period of time. Meanwhile, from the data below, the graphs have shown apparent relationships between the five variables and the polarization score.\n",
    "\n",
    "This report is separated into two parts. The first part is on some basic summary statistic tables and general graphs, which I use to refer to a general relationship between the Y and X variables. The second part goes into more detail on the graphs and maps, where I will present more specific visual and geographical relationships if they exist.\n",
    "\n",
    "*The data is taken by MANCH HUI, taken directly from Twitter. Downloaded from:\n",
    "https://www.kaggle.com/datasets/manchunhui/us-election-2020-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f678bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import necessary toolpacks needed\n",
    "import math\n",
    "import csv\n",
    "from math import isnan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "# import tools for mapping\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "# activate plot theme\n",
    "import qeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51adfa9c",
   "metadata": {},
   "source": [
    "## 1.2 Data cleaning\n",
    "Before doing any data analysis, we need to clean the data first.\n",
    "I first start with merging and X varialbes cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bde1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240600\n"
     ]
    }
   ],
   "source": [
    "# import the datasets and work on data cleaning\n",
    "dotru = pd.read_csv('/Users/gavinhao/Programming/UT_ECO225/Data/archive/hashtag_donaldtrump.csv',lineterminator='\\n')\n",
    "bid = pd.read_csv('/Users/gavinhao/Programming/UT_ECO225/Data/archive/hashtag_joebiden.csv',lineterminator='\\n')\n",
    "# merge the two datasets\n",
    "dotru[\"hashtag\"] = 'trump'\n",
    "bid[\"hashtag\"] = \"biden\"\n",
    "tweet = pd.concat([dotru,bid])\n",
    "# because we are merging by hashtag, there will be repeated values\n",
    "# checking the repeated value\n",
    "print(tweet.duplicated(subset=[\"tweet\"]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b966f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# so there are 240,600 repeated tweets, removing it:\n",
    "tweet.drop_duplicates(subset=[\"tweet\"], inplace=True, keep=False)\n",
    "print(tweet.duplicated(subset=[\"tweet\"]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc0f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, I also round data and time to the nearest day for created_at\n",
    "# we generated a new column for created_at, named as \"created_time\"\n",
    "tweet[\"created_at\"] = pd.to_datetime(tweet.created_at)\n",
    "# then, we rounded this object according to days\n",
    "tweet[\"created_time\"] = tweet[\"created_at\"].dt.round(\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20efcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, I also round data and time to the nearest year for user_join_date\n",
    "# we generated a new column for user_join_year, named as \"join_year\"\n",
    "tweet[\"user_join_date\"] = pd.to_datetime(tweet.user_join_date)\n",
    "# then, we rounded this object according to 365 days\n",
    "tweet[\"join_year\"] = tweet[\"user_join_date\"].dt.round(\"365D\")\n",
    "# then, we round join_year to year\n",
    "for i in range(len(tweet)):\n",
    "    timestamp = pd.Timestamp(tweet.iloc[i,23])\n",
    "    a = timestamp.to_pydatetime()\n",
    "    tweet.iloc[i,23] = a.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc63fae",
   "metadata": {},
   "source": [
    "Now we deal with Y variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calculating the polarity score, I am also interested in the topics that people are dicussing\n",
    "# Becasue topics that people engage in may show predictive effect of their polarity score\n",
    "# Therefore, I also get all the hashtags out of tweets for further research of my own interest\n",
    "# HOWEVER, for the entire file and codes to run, please ALSO run this sub-segmants of codes\n",
    "tweet[\"other_hash\"] = 'NaN' # at columns 24, create new variable\n",
    "for a in range(len(tweet)):\n",
    "    content = tweet.iloc[a,2].split()\n",
    "    for b in range(len(content)):\n",
    "        if content[b].startswith(\"#\"):\n",
    "            if tweet.iloc[a,24] == 'NaN':\n",
    "                tweet.iloc[a,24] = ''\n",
    "            cleaned_hashtag = content[b].strip(\":\").strip(\"?\").upper()\n",
    "            tweet.iloc[a,24] = tweet.iloc[a,24] + cleaned_hashtag +' '\n",
    "# WARNING: because we have almost 1,300,000 tweets in the merged file, this loop is likely going to\n",
    "# take you more than 4 mins to run - because it checks each single word in all \n",
    "# the tweets to identify the hashtags\n",
    "# it takes about 5 mins to run on MacBook with M1 Pro chip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e06a1",
   "metadata": {},
   "source": [
    "We now add a new column to show the polarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae71b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet[\"polar_score\"] = 0 # at columns 25, create new variable\n",
    "for a in range(len(tweet)):\n",
    "    score = TextBlob(tweet.iloc[a,2])\n",
    "    tweet.iloc[a,25] = score.sentiment.polarity\n",
    "# WARNING: because we have almost 1,300,000 tweets in the merged file, this loop is likely going to\n",
    "# take you more than 2 mins to run - because it checks each single word in all \n",
    "# the tweets to identify the value\n",
    "# it takes about 3 mins to run on MacBook with M1 Pro chip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d730846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the dataset\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46afbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.to_csv('/Users/gavinhao/Programming/UT_ECO225/Data/archive/sorted.csv')\n",
    "# Save a new file just in case we get lost somewhere\n",
    "# This file is likely to have a size of 700+ MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41416ced",
   "metadata": {},
   "source": [
    "Now the data is merged into one file called \"sorted,\" and all data are cleaned, we start digging into our research question.\n",
    "\n",
    "The cleaned dataframe is called \"tweet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0f726",
   "metadata": {},
   "source": [
    "## 1.3 Summary Statistic Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st X variable(created time) summary\n",
    "tweet[\"created_time\"].describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c58832",
   "metadata": {},
   "source": [
    "   The variable \"created_time\" refers to when the tweet is published on Twitter. The reason for choosing it is that we can relate the timing of a post to what's happening in the world, and thus we might be able to find some interesting facts. \n",
    "   \n",
    "   In this case, timing is even more critical because we use the 2020 US election data. As it moves closer to the time of the election, people's sentiments or polarization is likely to increase. Therefore, it is important to look at this relationship in our analysis; meanwhile, we can look at the data and see if it truly exists.\n",
    "\n",
    "   In this summary statistics table, we find that there are about 1.3 million timestamps recorded in 26 days(because we rounded dates by day when cleaning the data). The 31st of Oct in 2020 is the middle day according to the number of tweets published within the 26 days. With more than 1.3 million timestamps and a massive amount of sample size, I believe the data can represent reality fairly well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ed X variable(country) summary\n",
    "pd.DataFrame(tweet[\"country\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7677c8",
   "metadata": {},
   "source": [
    "   The variable \"country\" refers to the country where the tweet was published on Twitter. The reason for choosing it is because the different region has a different culture. This culture can hugely affect how people do and say things on Twitter. Thus, considering the effect of the country, we can relate to the sentiments of people around the world and compare if there are any significant differences.\n",
    "   \n",
    "   In this case, as we look into the 2020 US election data with hashtags of Trump and Biden, we will focus on how polarized people are worldwide. Will people in the United States be more polarized than people of other countries during this special time for US citizens? We will be able to answer this question using the country data.\n",
    "   \n",
    "   In this summary statistics table, a total of 188 countries are observed. And about 0.6 million tweets have country data available. The most frequently seen country in this dataset is the US, with more than 0.26 million tweets.\n",
    "   We have a huge amount of data in this sub-set, so in this research, I will randomly choose five countries from each continent except Antarctica because Antarctica needs more data. Hopefully, they can help us closely predict the parameter. In addition, because the US is most frequently observed, I will use another variable called \"state\" to study how polarization differs within the US alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f9862e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3rd X variable(followers) summary\n",
    "tweet[\"user_followers_count\"].describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743439d9",
   "metadata": {},
   "source": [
    "   The variable \"user_follower_count\" refers to the \"fans\" an account has on Twitter. The reason for choosing this is that we always debate whether popular people tend to be polarized or judgmental. Are those popular people all polarized, or are they less polarized and more open to other opinions? This topic relates to this research because people often get extreme in politics, so studying the effect of fans during this special period couldn't have better timing.\n",
    "   \n",
    "   In this summary statistics table, we find that, on average, an account has about 17k fans, with a huge stdev. Meaning this data is very positively skewed. Meanwhile, we can also tell that almost 75% of people don't have fans bigger than 2k. When dealing with this data, graphs might look messy. I have to be more careful about outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53364c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th X variable(join year) summary\n",
    "tweet[\"join_year\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953757e",
   "metadata": {},
   "source": [
    "   The variable \"join_year\" refers to the year the Twitter user signed up for the account. This section can be an excellent predictor of how old the person is or how much internet exposure this person has gotten. An interesting topic is: do people who join the internet longer will be more or less polarized on what they say? Again, as we are looking at the US election data, polarized people are more prominent, and thus it's easier for us to identify the effects.\n",
    "   \n",
    "   The summary statistics table shows that about 1.5 million people were registered not long ago in 2019. 2019 has been the highest registration in the years. All people in this dataset are registered across the 16 years horizon. However, 16 years is a long time, and registrations might be more linear. Thus, I should look for outliers and be careful when referring to certain relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th X variable(state) summary\n",
    "tweet[\"state\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d1fe7",
   "metadata": {},
   "source": [
    "   The variable \"state\" refers to which state the tweet is coming from. However, as mentioned above, this section is most used to identify major relationships between geographical relationships and polarization scores within the United States. So I will abstract the US states later from this sub-dataset. However, the reason for choosing this variable is the same: looking for a relationship regarding geographics.\n",
    "   \n",
    "   In this summary statistics table, we find that there are over 702 states recorded in the dataset. California is the most frequently observed state, with more than 45k tweets published there. Again, I will re-clean the data in the section below to get the data only from the United States for this research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y variable(polarity score) summary\n",
    "tweet[\"polar_score\"].describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776e46b",
   "metadata": {},
   "source": [
    "   This table is the summary statistics of the Y variable I chose. This score is the polarization score, as explained in the Introduction. \n",
    "   \n",
    "   The summary statistics table shows that the max and min of the score are 1 and -1, respectively. The mean score is 0.05, meaning people on Twitter during the US 2020 election that hashtagged trump and Biden are slightly positively polarized. For this research, we will carefully look into the score to identify possible relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f1046",
   "metadata": {},
   "source": [
    "## 1.4 Plots, Histograms, Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a7d35",
   "metadata": {},
   "source": [
    "#### Y variable histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Y variable (polarity score)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "tweet[\"polar_score\"].plot.hist(bins=10, alpha=1)\n",
    "ax.set_title(\"1a. Frequency Diagram of the Polarization Score\")\n",
    "ax.set_xlabel(\"Polarization Score\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67733c2",
   "metadata": {},
   "source": [
    "   We first explore how the Y variable looks in a histogram. This graph is a bi-model histogram, representing most of the polarity score lies between 0 and 0.25. This graph is consistent with the summary statistics above, with a mean of around 0.05, meaning that most people are lightly polarized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b50adb",
   "metadata": {},
   "source": [
    "For graphing analysis on X and Y variable, we need to re-classify the data.\n",
    "Becasue one country/state might have multiple polarity score associated, I need to take all scores out and average them, then put the number under the country/state, before I can draw any graphs.\n",
    "#### For X variables' relationship with Y variable graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For X variable: created_time\n",
    "c_time_country = {}\n",
    "for a in range(len(tweet)):\n",
    "    if tweet.iloc[a,22] not in c_time_country:\n",
    "        c_time_country[tweet.iloc[a,22]] = [tweet.iloc[a,25]]\n",
    "    elif tweet.iloc[a,22] in c_time_country:\n",
    "        c_time_country[tweet.iloc[a,22]].append(tweet.iloc[a,25])\n",
    "# Now take average of polarity score\n",
    "ave_c_time = {}\n",
    "for k,v in c_time_country.items():\n",
    "    # v is the list of polarity scores for time k\n",
    "    ave_c_time[k] = sum(v)/ float(len(v))\n",
    "#ave_c_time.pop(np.nan) # clean obs that don't have a corresponding country, \"nan\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39339ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we plot the diagram\n",
    "time_y = ave_c_time.keys()\n",
    "score_t = ave_c_time.values()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(time_y, score_t)\n",
    "ax.set_title(\"1b. Polarization Score of tweets sent in the one month period\")\n",
    "ax.set_ylabel(\"Polarization Score\")\n",
    "\n",
    "for label in ax.get_xticklabels():\n",
    "  label.set_rotation(75)\n",
    "  label.set_ha('right')\n",
    "    \n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326c2d6",
   "metadata": {},
   "source": [
    "   The reason for choosing this X variable is explained above in the summary statistics section.\n",
    "   In observing this graph, we see that, when it gets closer to the US election, people are getting more than more polarized across the one-month period. This graph follows our intuition as the debates over the election should become more aggressive over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For X variable: country\n",
    "dic_country = {}\n",
    "for a in range(len(tweet)):\n",
    "    if tweet.iloc[a,16] not in dic_country:\n",
    "        dic_country[tweet.iloc[a,16]] = [tweet.iloc[a,25]]\n",
    "    elif tweet.iloc[a,16] in dic_country:\n",
    "        dic_country[tweet.iloc[a,16]].append(tweet.iloc[a,25])\n",
    "# Now take average of polarity score\n",
    "ave_country = {}\n",
    "for k,v in dic_country.items():\n",
    "    # v is the list of polarity scores for country k\n",
    "    ave_country[k] = sum(v)/ float(len(v))\n",
    "ave_country.pop(np.nan) # clean obs that don't have a corresponding country, \"nan\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we have too many countries in this sample, darwing all 180 countries would be a mess\n",
    "# I will ramdonly choose 5 countries from 6 continents (except Antarctica), becasue Antarctica has little data \n",
    "# and hopefully they can help us closly predict the parameter\n",
    "# the 35 countries are in the list below\n",
    "country_list = [\"Japan\",\"India\",\"China\",\"Singapore\",\"Thailand\",\"Nigeria\",\"Ethiopia\",\"Egypt\", \\\n",
    "                \"Morocco\",\"Mali\",\"Panama\",\"Cuba\",\"Canada\",\"Mexico\",\"Dominica\",\"Peru\",\"Brazil\",\"Guyana\",\"Chile\",\\\n",
    "                \"Colombia\",\"Poland\",\"France\",\"Germany\",\"Austria\",\"Greece\",\\\n",
    "                \"Fiji\",\"New Zealand\",\"Australia\",\"Papua New Guinea\",\"Vanuatu\"]\n",
    "# Now we abstract data within the ave_state dictionary\n",
    "selc_ave_country = {}\n",
    "for i in country_list:\n",
    "    sel_c_score = ave_country[i]\n",
    "    selc_ave_country[i] = sel_c_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674138af",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = selc_ave_country.keys()\n",
    "score_country = selc_ave_country.values()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(country, score_country)\n",
    "ax.set_title(\"1c. Polarization Score of Tweets Sent in Different Countries\")\n",
    "ax.set_ylabel(\"Polarization Score\")\n",
    "ax.set_xlabel(\"Country\")\n",
    "# rotate the axis so we can see\n",
    "for label in ax.get_xticklabels():\n",
    "  label.set_rotation(75)\n",
    "  label.set_ha('right')\n",
    "    \n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "# notice for this grap, we have to also combine our knowledge of geographic distribution before formal interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e848599",
   "metadata": {},
   "source": [
    "   The reason for choosing this X variable is explained above in the summary statistics section.\n",
    "   Observing this graph, I found that tweets from Asian countries are more polarized because I ranked the countries according to their continents. In contrast, tweets in North America are overall less polarized. This phenomenon could result from a cultural difference at first glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For X variable: followers\n",
    "dic_followers = {}\n",
    "for a in range(len(tweet)):\n",
    "    if tweet.iloc[a,11] not in dic_followers:\n",
    "        dic_followers[tweet.iloc[a,11]] = [tweet.iloc[a,25]]\n",
    "    elif tweet.iloc[a,11] in dic_followers:\n",
    "        dic_followers[tweet.iloc[a,11]].append(tweet.iloc[a,25])\n",
    "# Now take average of polarity score\n",
    "ave_followers = {}\n",
    "for k,v in dic_followers.items():\n",
    "    # v is the list of polarity scores for country k\n",
    "    ave_followers[k] = sum(v)/ float(len(v))\n",
    "#ave_followers.pop(np.nan) # clean obs that don't have a corresponding country, \"nan\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# becasue X variables are not in order of smallest to largest, so I need to sort the keys\n",
    "followers_keys = list(ave_followers.keys())\n",
    "followers_keys.sort()\n",
    "sorted_followers = {i: ave_followers[i] for i in followers_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for better visualization, we drop the last 4 biggest outliers\n",
    "sorted_followers.pop(82396310.0)\n",
    "sorted_followers.pop(82396325.0)\n",
    "sorted_followers.pop(82417077.0)\n",
    "sorted_followers.pop(82417099.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847170cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_followers = sorted_followers.keys()\n",
    "score_followers = sorted_followers.values()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(c_followers, score_followers)\n",
    "ax.set_title(\"1d. Polarization Score of Different Number of Followers\")\n",
    "ax.set_ylabel(\"Polarization Score\")\n",
    "ax.set_xlabel(\"Count of Followers\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af198b04",
   "metadata": {},
   "source": [
    "   The reason for choosing this X variable is explained above in the summary statistics section.\n",
    "   Even though the scatters and the dots are spread out largly, there is still a sign of a positive relationship. This graph shows an upward-sloping relationship between fans owned and polarization score. One way to see this is through comparing the lowest polarization score given the count of followers. This graph says that people with more fans, or people more influential on Twitter, are likely to send out more polarized tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For X variable: join_year\n",
    "dic_j_year = {}\n",
    "for a in range(len(tweet)):\n",
    "    if tweet.iloc[a,23] not in dic_j_year:\n",
    "        dic_j_year[tweet.iloc[a,23]] = [tweet.iloc[a,25]]\n",
    "    elif tweet.iloc[a,23] in dic_j_year:\n",
    "        dic_j_year[tweet.iloc[a,23]].append(tweet.iloc[a,25])\n",
    "# Now take average of polarity score\n",
    "ave_j_year = {}\n",
    "for k,v in dic_j_year.items():\n",
    "    # v is the list of polarity scores for country k\n",
    "    ave_j_year[k] = sum(v)/ float(len(v))\n",
    "ave_j_year.pop(1970) # pop out the outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_year_keys = list(ave_j_year.keys())\n",
    "join_year_keys.sort()\n",
    "sorted_ave_j_year = {i: ave_j_year[i] for i in join_year_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a10b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start drawing the graph\n",
    "join_year = sorted_ave_j_year.keys()\n",
    "score_join = sorted_ave_j_year.values()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(join_year, score_join)\n",
    "ax.set_title(\"1e. Polarization Score of Account Created in Different Years\")\n",
    "ax.set_ylabel(\"Polarization Score\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a0739",
   "metadata": {},
   "source": [
    "   The reason for choosing this X variable is explained above in the summary statistics section.\n",
    "   In observing this graph, we see an upward-sloping relationship between join_year and polarization score. So this graph is saying that people who registered later, people who have less internet experience, or people who are likely to be younger are likely to send out more polarized tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For X variable: state\n",
    "dic_state = {}\n",
    "for a in range(len(tweet)):\n",
    "    if tweet.iloc[a,18] not in dic_state and tweet.iloc[a,16] == \"United States of America\":\n",
    "        dic_state[tweet.iloc[a,18]] = [tweet.iloc[a,25]]\n",
    "    elif tweet.iloc[a,18] in dic_state and tweet.iloc[a,16] == \"United States of America\":\n",
    "        dic_state[tweet.iloc[a,18]].append(tweet.iloc[a,25])\n",
    "# Now take average of polarity score\n",
    "ave_state = {}\n",
    "for k,v in dic_state.items():\n",
    "    # v is the list of polarity scores for country k\n",
    "    ave_state[k] = sum(v)/ float(len(v))\n",
    "ave_state.pop(np.nan) # clean obs that don't have a corresponding state, \"nan\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cec51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# becasue there are too many states, we will focus on the top 20 biggest states in the United States by population\n",
    "# those are: California, Texas, Florida, New York, Pennsylvania, Illinois, Ohio, Georgia, North Carolina, \n",
    "# Michigan, New Jersey, Virginia, Washington, Arizona, Massachusetts, Tennessee, Indiana, Maryland, Missouri\n",
    "# and Colorado\n",
    "# List taken from: https://worldpopulationreview.com/states\n",
    "state_list = [\"California\",\"Texas\",\"Florida\",\"New York\",\"Pennsylvania\",\"Illinois\",\"Ohio\",\"Georgia\", \\\n",
    "              \"North Carolina\",\"Michigan\",\"New Jersey\",\"Virginia\",\"Washington\",\"Arizona\",\"Massachusetts\",\\\n",
    "              \"Tennessee\",\"Indiana\",\"Maryland\",\"Missouri\",\"Colorado\"]\n",
    "# Now we abstract data within the ave_state dictionary\n",
    "us_ave_state = {}\n",
    "for i in state_list:\n",
    "    state_score = ave_state[i]\n",
    "    us_ave_state[i] = state_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can plot the graph\n",
    "the_state = us_ave_state.keys()\n",
    "score_state = us_ave_state.values()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(the_state, score_state)\n",
    "ax.set_title(\"1f. Polarization Score of Tweets Sent in Different States\")\n",
    "ax.set_ylabel(\"Polarization Score\")\n",
    "ax.set_xlabel(\"State\")\n",
    "# rotate the axis so we can see\n",
    "for label in ax.get_xticklabels():\n",
    "  label.set_rotation(75)\n",
    "  label.set_ha('right')\n",
    "    \n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "# notice for this grap, we have to also combine our knowledge of geographic distribution before formal interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a83cf",
   "metadata": {},
   "source": [
    "   The reason for choosing this X variable is explained above in the summary statistics section.\n",
    "   Observing this graph, we have to consider the location of where the states. After taking into the effect of that, we see that people who live on the east side of the United States tend to send more polarized tweets than those on the west side. I suspect it's because of the cultural difference and people's living habits between the two coasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ef4ce",
   "metadata": {},
   "source": [
    "# 2. Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea4819",
   "metadata": {},
   "source": [
    "## 2.1 The Message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9085558",
   "metadata": {},
   "source": [
    "From the above general graphs, we get to know that the following relationships may exist between the X and Y variables:\n",
    "\n",
    "- People get more polarized when it gets closer to the election date (created_time)\n",
    "- Residents in the east of the US tend to send out more positive tweets than that in the west (states)\n",
    "- Asian countries tend to have more positive sentiments than other continents (countries)\n",
    "- Younger people tend to send out more positive content (join_year)\n",
    "- The more fans an account has, the more positive content it sends (fans_owned)\n",
    "\n",
    "Because in the above graphs, we already see a significant relationship for X variable \"time\"(tweet posted time). Thus, in the following section, I will focus on the other four variables where I can dig deeper and see if the true significant relationships exist.\n",
    "We will now dig into the details and draw better graphs and maps to show the remaining relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb5e83",
   "metadata": {},
   "source": [
    "### 2.1.1 Detailed Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc36e3",
   "metadata": {},
   "source": [
    "#### Polarization socre VS. location of the tweet within the US"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856877b",
   "metadata": {},
   "source": [
    "We draw two graphs to explore whether and by how much people's polarization scores differ across the United States. Two graphs show the ten states with the highest and lowest polarization scores, respectively. We start with the highest polarization score and then the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frist I sort the values of polarization score from highest to lowest and vice versa\n",
    "# This dictionary is taken from Project 1 which was already cleaned and taken from the raw dataset\n",
    "ave_state = dict(sorted(ave_state.items(), key=lambda item: item[1]))\n",
    "ave_state_low = dict(list(ave_state.items())[1:11])\n",
    "ave_state_high = dict(list(ave_state.items())[-11:-1])\n",
    "# I drop one lowest and one highest for better visualization and potential errors\n",
    "# now transfer low to dataframe object\n",
    "low_state = pd.DataFrame(ave_state_low, index=['0']).transpose()\n",
    "low_state = low_state.rename(columns={'0': 'Polarization Score'})\n",
    "# now transfer high to dataframe object\n",
    "high_state = pd.DataFrame(ave_state_high, index=['0']).transpose()\n",
    "high_state = high_state.rename(columns={'0': 'Polarization Score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "\n",
    "# create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# plot data on first subplot\n",
    "low_state[\"Polarization Score\"].plot(kind=\"barh\", ax=ax1, color=colorsys.hls_to_rgb(0.5, 0.75, 0.9))\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.set_title(\"US States with the Lowest Polarization Score\")\n",
    "ax1.tick_params(axis='y', labelsize=9)  # set font size for y-axis\n",
    "ax1.set_xlabel(\"Polarization Score\")  # set label for x-axis\n",
    "ax1.set_xlim([0, 0.08])  # set x range to better compare relatively\n",
    "\n",
    "# plot data on second subplot\n",
    "high_state[\"Polarization Score\"].plot(kind=\"barh\", ax=ax2, color=colorsys.hls_to_rgb(0, 0.3, 0.9))\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.set_title(\"US States with the Highest Polarization Score\")\n",
    "ax2.tick_params(axis='y', labelsize=9)  # set font size for y-axis\n",
    "ax2.set_xlabel(\"Polarization Score\")  # set label for x-axis\n",
    "# saturation of colour 0.3 and 0.75 should work \n",
    "# whether you have a black-and-white printer or a coloured printer\n",
    "\n",
    "\n",
    "# add a title to the figure\n",
    "fig.suptitle('2.1.1a Comparison of Polarization Scores Across US States')\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54e47d",
   "metadata": {},
   "source": [
    "We can refer to some interesting conclusions from this graph:\n",
    "- Even the lowest-scoring states have scores above 0, meaning people in the US are at least somewhat positively polarized.\n",
    "- I intentionally set both axes into the same scale of 0.08 to better compare - and it turns out that the difference between the highest and lowest scores is not big(0.05 vs 0.08). Thus, we should not conclude that there is a significant difference in the polarization score across the country.\n",
    "- However, if we are to compare, we find that most high-scoring states are in the east of the US, while lots of low-scoring states are in the south. So maybe we can refer to the fact that residents in the east of the US tend to send out more positive tweets than those in the west.\n",
    "\n",
    "However, we will address this question better in the below mapping section, where I directly show the map for the polarization within the US. I am offering this graph here so we can have different perspectives to see if the phenomenon we observe is valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b2fa2",
   "metadata": {},
   "source": [
    "#### Polarization socre VS. User Registered Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15261199",
   "metadata": {},
   "source": [
    "To explore closer on whether and by how much people's polarization scores differ according to their registered time, we draw the following graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data already sorted above to dataframe object\n",
    "j_time_df = pd.DataFrame(sorted_ave_j_year, index=['0']).transpose()\n",
    "j_time_df = j_time_df.rename(columns={'0': 'Polarization Score'})\n",
    "j_time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaeb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "colors = [\"r\" if x < 0 else \"k\" for x in j_time_df[\"Polarization Score\"].values] # define colour\n",
    "bars = ax.bar(np.arange(len(colors)), j_time_df[\"Polarization Score\"].values, color=colors, alpha=0.8)\n",
    "ax.hlines(0, 0, 14.5) # get the bottom line of the graph\n",
    "ax.set_xticks(np.arange(len(j_time_df.index)))\n",
    "ax.set_xticklabels(j_time_df.index)\n",
    "ax.tick_params(axis='x', labelrotation=30)# rotate the x-axis so we see better\n",
    "\n",
    "ax.set_title(\"2.1.1b Polarization Score of Different Account Registered Time\")\n",
    "# set the axis\n",
    "plt.ylabel(\"Polarization Score\")\n",
    "plt.xlabel(\"Year\")\n",
    "# remove the boundaries\n",
    "for _spine in [\"right\", \"top\", \"left\", \"bottom\"]:\n",
    "    ax.spines[_spine].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a274b0",
   "metadata": {},
   "source": [
    "This graph proves the otherwise to the conclusion we made above. In the scatter plot in Project 1, we see a slight increase in the polarization score as the account registered time grows - I thought that made sense because I thought intuitively that younger people tend to be more polarized. However, there was little of a big difference in this bar plot. The potential reason could be the misleading scale of the y-axis. \n",
    "\n",
    "The polarization score overall is very stable across the account registered year in this bar plot. Whether a person has larger or smaller exposure to Twitter will not essentially affect how polarized this person is on social media.\n",
    "\n",
    "However, the variable \"registered time\" is ambiguous and could be connected to various factors. For example, we cannot see clear relationships in this bar plot. Is it because our sample is too mixed, and we should also control other variables? \n",
    "\n",
    "Therefore, I will create a map below for this X variable in the mainland United States exclusively, narrowing down the data range and controlling for other potential lurking variables - just to see how the registered time correlate to location and polarization score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5aafc5",
   "metadata": {},
   "source": [
    "#### Polarization socre VS. Number of Fans Owned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa4f00c",
   "metadata": {},
   "source": [
    "To explore closer whether and by how much people's polarization scores differ with the different numbers of followers, I update the graph from Project 1. Because a single scatter plot makes people hard to identify the trend, I will add a trend line onto the graph and see if the relationship between the X and Y truly exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3840c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_followers = list(sorted_followers.keys())\n",
    "score_followers = list(sorted_followers.values())\n",
    "\n",
    "# Fit a polynomial line of degree 1 (i.e., a straight line) to the data\n",
    "z = np.polyfit(c_followers, score_followers, 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# Plot the scatter plot and the trend line\n",
    "plt.scatter(c_followers, score_followers)\n",
    "plt.plot(c_followers, p(c_followers), \"-\", color=\"red\", linewidth=3)\n",
    "plt.title(\"2.1.1c Polarization Score of Different Number of Followers\")\n",
    "plt.ylabel(\"Polarization Score\")\n",
    "plt.xlabel(\"Count of Followers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767a450",
   "metadata": {},
   "source": [
    "Thus, our conclusion in project 1 is likely wrong. In project 1's findings, I inferred that people with more fans tend to be more polarized by visualization. \n",
    "\n",
    "However, with the help of an almost perfect horizontal trend line, we should infer that:\n",
    "- There isn't sufficient proof to say a significant or apparent relationship exists between fans owned and polarization score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e79ddc",
   "metadata": {},
   "source": [
    "## 2.2 Maps and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908b91a",
   "metadata": {},
   "source": [
    "### 2.2.1 Map for Polarization Within the US (X: States in the US, Y: Polarization Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will first get the sorted list of the polarization score and the US state list here first\n",
    "# I will use low_state dataframe just for convience\n",
    "map_ave_state = pd.DataFrame(ave_state, index=['0']).transpose()\n",
    "map_ave_state = map_ave_state.rename(columns={'0': 'Polarization Score'})\n",
    "map_ave_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51493c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we get the state gpd data\n",
    "state_df = gpd.read_file(\"http://www2.census.gov/geo/tiger/GENZ2016/shp/cb_2016_us_state_5m.zip\")\n",
    "# now we merge the two files\n",
    "merge_state_map = state_df.merge(map_ave_state, left_on=\"NAME\", right_index=True, how=\"inner\")\n",
    "# drop a few outliers due to small amount of observations\n",
    "# and we will focus on only the mainland states\n",
    "# because only mainland states concern the research questions\n",
    "condition_state_1 = (state_df['NAME'] == 'Guam') # specify the condition to delete\n",
    "merge_state_map = merge_state_map.drop(state_df[condition_state_1].index)\n",
    "condition_state_3 = (state_df['NAME'] == 'Hawaii') # specify the condition to delete\n",
    "merge_state_map = merge_state_map.drop(state_df[condition_state_3].index)\n",
    "condition_state_2 = merge_state_map.bounds['maxx'] >= 165\n",
    "merge_state_map = merge_state_map.loc[~condition_state_2] # delete rows that satisfy the condition\n",
    "condition_state_4 = merge_state_map.bounds['miny'] <= 20\n",
    "merge_state_map = merge_state_map.loc[~condition_state_4] # delete rows that satisfy the condition\n",
    "# last but not least, we want to standardize the polarization score so it's from -1 to 1\n",
    "merge_state_map['norm_polar'] = (2*(merge_state_map['Polarization Score']-\\\n",
    "                                    merge_state_map['Polarization Score'].min()) \\\n",
    "                             / (merge_state_map['Polarization Score'].max()-\\\n",
    "                                merge_state_map['Polarization Score'].min())) - 1\n",
    "# now take a look at the gpd file\n",
    "merge_state_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we start to graph\n",
    "# start with the U.S. map layer\n",
    "fig, gax = plt.subplots(figsize=(20,10))\n",
    "merge_state_map.plot(ax=gax, edgecolor='black',color='white')\n",
    "# then we map the colour of polarization score\n",
    "merge_state_map.plot(\n",
    "    ax=gax, edgecolor='black', column=\"Polarization Score\", legend=False, cmap='RdBu_r',\n",
    ")\n",
    "plt.title('2.2.1a Polarization Score Across the US States', fontsize = 25)\n",
    "plt.axis('off')\n",
    "\n",
    "# we now create a colorbar\n",
    "fig, ax = plt.subplots(figsize=(13, 1))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "cmap = 'RdBu_r'\n",
    "norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "             cax=ax, orientation='horizontal', label='Polarization Score')\n",
    "\n",
    "# end of colorbar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a1fc5",
   "metadata": {},
   "source": [
    "This map shows a better visualization of the polarization score across the United States. Meanwhile, we inferred from the previous part that: we should not conclude that there is a significant difference in the polarization score across the country, and if we are to compare, we find that most high-scoring states are in the east of the US, while lots of low scoring states are in the south.\n",
    "\n",
    "The previous conclusion that \"there isn't a significant difference in the polarization score across the country\" is true, as most states are positively polarized across the United States. However, the conclusion that \"most high-scoring states are in the east of the US, while lots of low-scoring states are in the south\" might not be true. Low-scoring states are mostly in the southwest instead of the west, while the eastern states tend to be more polarized overall.\n",
    "\n",
    "Thus, the official conclusion regarding polarization score and the geographical location within the United States is as follows:\n",
    "- We should not conclude that there is a significant difference in the polarization score across the country\n",
    "- However, comparatively, low polarization scoring states are mostly in the southwest, while the eastern states tend to be more polarized overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070e0ca",
   "metadata": {},
   "source": [
    "### 2.2.2 Map for Account Created Time (X: Average time of Year when the account was created in the US)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83fbbda",
   "metadata": {},
   "source": [
    "The only two relationships left to dig deeper into are the following:\n",
    "- Asian countries tend to have more positive sentiments than other continents (countries)\n",
    "- Younger people tend to send out more positive content (join_year)\n",
    "I will use the remaining two maps to research more on those relationships.\n",
    "\n",
    "Meanwhile, as mentioned above: \n",
    "\n",
    "The variable \"registered time\" is ambiguous and could be connected to various factors. For example, we cannot see clear relationships in the above bar plot. Is it because our sample is too mixed, and we should also control for other variables? \n",
    "\n",
    "Therefore, I am going to create a map here for this X variable(registered time) in the mainland United States exclusively, narrowing down the data range and controlling for other potential lurking variables - to see how the registered time correlate to location and polarization score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfdf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For X variable: join_year\n",
    "dic_j_year_map = {}\n",
    "for a in range(len(tweet)):\n",
    "    if tweet.iloc[a,18] not in dic_j_year_map and tweet.iloc[a,16] == \"United States of America\":\n",
    "        dic_j_year_map[tweet.iloc[a,18]] = [tweet.iloc[a,-3]]\n",
    "    elif tweet.iloc[a,18] in dic_j_year_map and tweet.iloc[a,16] == \"United States of America\":\n",
    "        dic_j_year_map[tweet.iloc[a,18]].append(tweet.iloc[a,-3])\n",
    "# Now take average of year\n",
    "ave_j_year_map = {}\n",
    "for k,v in dic_j_year_map.items():\n",
    "    # v is the list of average created year rounded for country k\n",
    "    ave_j_year_map[k] = round(sum(v)/ float(len(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e308d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the list into a pandas object\n",
    "state_year_map = pd.DataFrame(ave_j_year_map, index=['0']).transpose()\n",
    "state_year_map = state_year_map.rename(columns={'0': 'Ave Registered Year'})\n",
    "state_year_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we already have a US mainland map editted\n",
    "# I will merge this new data into it and start drawing\n",
    "merge_state_map = merge_state_map.merge(state_year_map, left_on=\"NAME\", right_index=True, how=\"inner\")\n",
    "merge_state_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c136d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now we start to graph\n",
    "fig, gax = plt.subplots(figsize=(20,10))\n",
    "merge_state_map.plot(ax=gax, edgecolor='black',color='white')\n",
    "# then we map the colour of polarization score\n",
    "merge_state_map.plot(\n",
    "    ax=gax, edgecolor='black', column=\"Ave Registered Year\", legend=False, cmap='RdBu_r',\n",
    ")\n",
    "plt.title('2.2.2a Ave Registered Year Across the US States', fontsize = 25)\n",
    "plt.axis('off')\n",
    "\n",
    "# we now create a colorbar\n",
    "fig, ax = plt.subplots(figsize=(13, 1))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "cmap = 'RdBu_r'\n",
    "norm = mpl.colors.Normalize(vmin=2011, vmax=2015)\n",
    "\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "             cax=ax, orientation='horizontal', label='Year')\n",
    "\n",
    "# show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37c371",
   "metadata": {},
   "source": [
    "From this graph alone, we can identify that most states' average Twitter accounts registered time ranges from 2012 to 2014. Meanwhile, most western states have a registered time of 2013, while many middle and eastern states have a registered time of 2011.\n",
    "\n",
    "However, this map alone does not tell any correlations or relationships. Thus, I will draw the two maps together to see potential relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I want to draw this map with the polarization map together\n",
    "# so I can see potential relationships\n",
    "\n",
    "import matplotlib as mpl\n",
    "# create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20,10))\n",
    "\n",
    "# plot Ave Registered Year on the first subplot\n",
    "merge_state_map.plot(ax=ax1, edgecolor='black', color='white')\n",
    "merge_state_map.plot(ax=ax1, edgecolor='black', column=\"Ave Registered Year\", legend=False, cmap='RdBu_r')\n",
    "ax1.set_title('2.2.2b Ave Registered Year Across the US States', fontsize=22)\n",
    "ax1.axis('off')\n",
    "# plot Polarization Score on the second subplot\n",
    "merge_state_map.plot(ax=ax2, edgecolor='black', color='white')\n",
    "merge_state_map.plot(ax=ax2, edgecolor='black', column=\"Polarization Score\", legend=False, cmap='RdBu_r')\n",
    "ax2.set_title('2.2.2c Polarization Score Across the US States', fontsize=22)\n",
    "ax2.axis('off')\n",
    "\n",
    "# create a colorbar for the first map\n",
    "fig.colorbar(ax1.collections[1], ax=ax1, orientation='horizontal', label='Ave Registered Year')\n",
    "# create a colorbar for the second map\n",
    "fig.colorbar(ax2.collections[1], ax=ax2, orientation='horizontal', label='Polarization Score')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7013a",
   "metadata": {},
   "source": [
    "Here are two graphs together, the one on the left is a US map with a different Twitter account registered time; the one to the right is the US map with varying polarization scores.\n",
    "\n",
    "In the previous parts, with graph 2.1.1b, we conclude that there isn't any strong relationship between the polarization score and the account-created time. However, when comparing these two graphs, there is some correlation.\n",
    "\n",
    "We can see that most states that are more polarized(red in the right graph) were registered around 2013(white in the left graph); most states that are less polarized(light blue in the right graph) were registered around 2011(deep blue in the left graph); especially when we look at the states in the south-west. Thus, we can infer that: the later the account is created, the more polarized the posted content is. This conclusion aligns with the observation in graph 1e.\n",
    "\n",
    "The potential reason for observing a trend here could be mentioned above: there might be unobserved lurking variables. We get to see a better relationship as I narrow down the data range and potentially eliminate the effect of those variables.\n",
    "\n",
    "However, to dig deeper, we must run a multi-regression model and control for other factors to see the real trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b814c8",
   "metadata": {},
   "source": [
    "### 2.2.3 Maps for Polarization Acorss the World (X: Countries in the World, Y: Polarization Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e16efa",
   "metadata": {},
   "source": [
    "To research on the last relationship, polarization score vs different countries, I draw a world map to show potential relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af25713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the following code to generate 1 polarization score for 1 country \n",
    "# (taking average of multiple scores)\n",
    "# For X variable: country\n",
    "dic_country_map = {}\n",
    "for a in range(len(tweet)):\n",
    "    if tweet.iloc[a,16] not in dic_country_map:\n",
    "        dic_country_map[tweet.iloc[a,16]] = [tweet.iloc[a,25]]\n",
    "    elif tweet.iloc[a,16] in dic_country_map:\n",
    "        dic_country_map[tweet.iloc[a,16]].append(tweet.iloc[a,25])\n",
    "# Now take average of polarity score\n",
    "ave_country_map = {}\n",
    "for k,v in dic_country_map.items():\n",
    "    # v is the list of polarity scores for country k\n",
    "    ave_country_map[k] = sum(v)/ float(len(v))\n",
    "ave_country_map.pop(np.nan) # clean obs that don't have a corresponding country, \"nan\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82db638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the dictionary into a list first, so we can manually change some data to better merge the datasets\n",
    "country_keys = list(ave_country_map.keys())\n",
    "country_values = list(ave_country_map.values())\n",
    "# there are two countries that hvae different names in the two dataset\n",
    "# so I am going to change the name in the tweet dataset to match the other one\n",
    "for i in range(len(country_keys)):\n",
    "    if country_keys[i] == \"Republic of the Congo\":\n",
    "        country_keys[i] = \"Congo\"\n",
    "    elif country_keys[i] == \" South Sudan\":\n",
    "        country_keys[i] = \"S. Sudan\"\n",
    "# combine the two lists and generate a dataframe object\n",
    "whole_country_list = [country_keys] + [country_values]\n",
    "country_map = pd.DataFrame(whole_country_list, index=['country','polarization']).transpose()\n",
    "# drop a few outliers due to small tweet observations\n",
    "country_map = country_map.drop(132)\n",
    "country_map = country_map.drop(186)\n",
    "country_map = country_map.drop(184)\n",
    "country_map = country_map.drop(179)\n",
    "# we want to normalize the value from -1 to 1\n",
    "country_map['norm_polar'] = (2*(country_map['polarization']-country_map['polarization'].min()) \\\n",
    "                             / (country_map['polarization'].max()-country_map['polarization'].min())) - 1\n",
    "country_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb595d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we import the world map and start drawing\n",
    "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe74e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we put the ploarization score into the world data\n",
    "country_polar_map = world.merge(country_map, left_on=\"name\", right_on=\"country\", how=\"inner\")\n",
    "country_polar_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we start to graph\n",
    "# start with the world map layer\n",
    "fig, gax = plt.subplots(figsize=(20,10))\n",
    "world.plot(ax=gax, edgecolor='black',color='white')\n",
    "# then we map the colour of polarization score\n",
    "country_polar_map.plot(\n",
    "    ax=gax, edgecolor='black', column=\"norm_polar\", legend=False, cmap='RdBu_r',\n",
    ")\n",
    "plt.title('2.2.3a Polarization Score Across the World', fontsize = 25)\n",
    "plt.axis('off')\n",
    "# we now create a colorbar\n",
    "fig, ax = plt.subplots(figsize=(13, 1))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "cmap = 'RdBu_r'\n",
    "norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "             cax=ax, orientation='horizontal', label='Polarization Score')\n",
    "# end of colorbar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65f55f",
   "metadata": {},
   "source": [
    "This graph shows that polarizations of tweets across the world have huge differences. North America tends to have more positive tweets, while South America and Europe tend to have more negative tweets. While Australian countries were in the middle ground - not too positive nor negative, African countries had mixed polarization scores across the continent with some missing observations(as no tweets are posted from those countries).\n",
    "\n",
    "This conclusion aligns with the conclusion in project 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e309ff",
   "metadata": {},
   "source": [
    "## 2.3 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058542c",
   "metadata": {},
   "source": [
    "From the above graphs and maps, although some graphs contradict each other, we can still infer several important conclusions. The conclusions are: when it gets closer to the US election, people are getting more than more polarized; the polarization score difference is not significant across the US, but comparatively, most high-scoring states are in the east of the US, while lots of low scoring states are in the south; Asian countries tend to have more positive sentiments than other continents; and that there are no significant relationships between fans owned and polarization score. \n",
    "\n",
    "However, one X variable exhibits an ambiguous relationship to the polarization score. The X variable is account registered time, which is an indicator of the user's previous exposure to the internet or the person's age. The relationship is ambiguous because there is a high chance this variable is affected by another lurking variable. I tried to eliminate the effects by narrowing the range in the above map. Still, I will need more tools like muti-regression to derive a more specific conclusion. Thus, the conclusion up to this point is that this X variable(user join date) exhibits an ambiguous relationship to the polarization score.\n",
    "\n",
    "Still, even though we believe the conclusions on the other four X variables are well-supported, we must be aware of the potential existence of the lurking variables. We can better control those lurking variables if we find an approach for a multi-regression model for all X variables. Meanwhile, even though TextBlob is believed to accurately identify the polarization score on people's speeches, AI and natural language processing is still a developing field. Therefore, we should be more careful when processing those data.\n",
    "   \n",
    "In this report and dataset, we only observed the relationships to certain important X variables. In the future, we could explore how those polarization scores reflect people's emotions or living standards; and find some correlations or predictions between the five variables and people's emotional standards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
